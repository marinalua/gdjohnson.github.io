<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	xmlns:georss="http://www.georss.org/georss" xmlns:geo="http://www.w3.org/2003/01/geo/wgs84_pos#" xmlns:media="http://search.yahoo.com/mrss/"
	>

<channel>
	<title>Matt Simpson &#8211; Carcinisation</title>
	<atom:link href="https://carcinisation.com/author/spoolinator/feed/" rel="self" type="application/rss+xml" />
	<link>https://carcinisation.com</link>
	<description>One of nature&#039;s many attempts to evolve a crab.</description>
	<lastBuildDate>Thu, 15 Jan 2015 22:49:42 +0000</lastBuildDate>
	<language>en</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>http://wordpress.com/</generator>

<image>
	<url>https://carcinisation.files.wordpress.com/2017/08/cropped-crab.jpg?w=32</url>
	<title>Matt Simpson &#8211; Carcinisation</title>
	<link>https://carcinisation.com</link>
	<width>32</width>
	<height>32</height>
</image> 
<cloud domain='carcinisation.com' port='80' path='/?rsscloud=notify' registerProcedure='' protocol='http-post' />
<atom:link rel="search" type="application/opensearchdescription+xml" href="https://carcinisation.com/osd.xml" title="Carcinisation" />
	<atom:link rel='hub' href='https://carcinisation.com/?pushpress=hub'/>
	<item>
		<title>In Order To Give A Man A Fish, You Must First Teach Him How To Eat It</title>
		<link>https://carcinisation.com/2015/01/15/in-order-to-give-a-man-a-fish-you-must-teach-him-how-to-eat-it/</link>
				<comments>https://carcinisation.com/2015/01/15/in-order-to-give-a-man-a-fish-you-must-teach-him-how-to-eat-it/#comments</comments>
				<pubDate>Thu, 15 Jan 2015 22:48:57 +0000</pubDate>
		<dc:creator><![CDATA[Matt Simpson]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[art]]></category>

		<guid isPermaLink="false">http://carcinisation.com/?p=375</guid>
				<description><![CDATA[A couple weeks ago I got to spend an evening with Haley and Gabe in Boston discussing various things that probably bored Haley to tears (sorry!), but at least Gabe found interesting. One of Gabe&#8217;s shticks is that art should do a better job of conveying important ideas and concepts to its consumers. His favorite example &#8230; <a href="https://carcinisation.com/2015/01/15/in-order-to-give-a-man-a-fish-you-must-teach-him-how-to-eat-it/" class="more-link">Continue reading<span class="screen-reader-text"> "In Order To Give A Man A Fish, You Must First Teach Him How To Eat&#160;It"</span></a>]]></description>
								<content:encoded><![CDATA[<p>A couple weeks ago I got to spend an evening with <a title="https://twitter.com/thesublemon" href="https://twitter.com/thesublemon">Haley</a> and <a title="https://twitter.com/GabrielDuquette" href="https://twitter.com/GabrielDuquette">Gabe</a> in Boston discussing various things that probably bored Haley to tears (sorry!), but at least Gabe found interesting. One of Gabe&#8217;s shticks is that art should do a better job of conveying important ideas and concepts to its consumers. His favorite example of this (of everything?) is The Wire.</p>
<p>I want to push back on this a little bit &#8211; an alternate title this post might be &#8220;In Which I Attempt To Troll Gabe Into Coming Back To The Internet At Least In Small Doses And In A Totally Responsible Fashion Which Won&#8217;t Disrupt His Life And Waste His Time And Also Into Maybe Writing Something Interesting&#8221; but it just didn&#8217;t seem catchy enough.</p>
<p>Let&#8217;s take the premise at face value &#8211; that art should try to convey useful, meaningful concepts to its consumers. This seems reasonable as at least one of the things art should do. Well, it&#8217;s reasonable so long as art <em>can</em> convey these concepts. But can it? At face value, the answer is obvious &#8211; of course it can! The Matrix taught a generation that their perceived reality is possibly not even real. Inception did this for another generation.</p>
<p>But this dodges the question by avoiding the steel man: can art realistically convey new concepts to people often? Or efficiently? It&#8217;s easy to be less sanguine about this question.</p>
<p>A good example here is a concept from economics (and elsewhere) called elasticity. The price elasticity of demand, for example, is the % change in the quantity of a good demanded caused by a 1 % change in the price. There&#8217;s an ambiguity here though &#8211; are these percentages computed by using the original price and quantity as the base? The new price and quantity? Or maybe the average of the new and old price and quantities? Different textbooks and instructors make different decisions here, and students often get lost in a formula that doesn&#8217;t at all seem intuitive. Conveying the concept of elasticity at a high enough resolution that the recipient can understand the math is hard.</p>
<p>Unless they already know calculus. In terms of calculus, price elasticity of demand is dQ/dP * (P/Q), or in other words the derivative of demand with respect to price times the ration of price to demand. We can rearrange this equation as (dQ/Q)/(dP/P) in order to make it look more like percentage changes. If a student already knows calculus, it&#8217;s often very easy to explain elasticity to them &#8211; the ambiguity about base percentages goes away through the magic of calculus, and learning calculus already taught them about rates of changes. It&#8217;s also really easy to explain acceleration (the derivative of velocity) or jerk (the derivative of acceleration) or a whole host of useful concepts from a wide variety of disciplines.</p>
<p>I&#8217;m not just saying it&#8217;s better to teach a man to fish than to give a man a fish &#8211; as Gabe told me in Boston there&#8217;s some low hanging fruit to be grabbed just from giving people fish, and I agree. Some of the people you give fish to may have starved after all! But there&#8217;s something else going on here &#8211; not everyone knows how to eat fish. The fish eating novice might not chew thoroughly and miss the small bone tucked in the flaky goodness, resulting in some discomfort, post traumatic stress syndrome, and a lifetime devoid of fish. This, I contend, is a potential problem with trying to give someone the concept of elasticity who has never had calculus. Fundamentally, this is a really hard thing to do in such a way that requires no work on the recipient&#8217;s end.</p>
<p>Perhaps this is why Gabe reveres art which successfully conveys important, meaningful, and difficult concepts well &#8211; precisely because it&#8217;s so difficult. But I must ask the question, was is really that successful? Or did the people who got the concept already have the equivalent of calculus under their belt? Consider The Wire. As <a title="http://www.overcomingbias.com/2008/10/the-wire.html" href="http://www.overcomingbias.com/2008/10/the-wire.html">Robin Hanson</a> notes:</p>
<blockquote><p>The overall moral of the story seems to me largely libertarian.  A renegade cop effectively legalizing drugs in one area works out great, and the show’s writers have a <a href="http://www.time.com/time/nation/article/0,8599,1719872,00.html"><em>Time</em> oped</a> supporting drug law jury nullification.  Dire consequences follow from child labor and prostitution being illegal.  The police, courts, prisons, schools, and city hall are unrelentingly corrupt and dysfunctional, because voters don’t much care.  In the background of the story, industries managed mainly by private enterprise, such as stores, hotels, shipping, and cars, seem to mostly function well.  Private newspapers look bad, but mainly because readers don’t much care.</p>
<p>Apparently, however, many see <em>The Wired</em> as calling for more government.  At a <a href="http://www.iop.harvard.edu/Multimedia-Center/All-Videos/The-HBO-Series-The-Wire-A-Compelling-Portrayal-of-an-American-City">Harvard symposium</a> on <em>The Wired</em>, many panelists said the answer was more funding.  Simon was there:</p>
<blockquote><p>The wire is about a world in which people are worth less. … We depicted a world in which market forces always have their say and in which capitalism has triumphed, and marginalized labor – it makes labor cheap. … What we have here is a market-based [world]; capitalism has been the God.  To even suggest that there should be some social compact along with the capitalistic forces, to mitigate any of that, over the last twenty-five years, has been political suicide. … We are only getting the American that we’ve paid for, no more, and God damn it, we deserve it.</p></blockquote>
<p>&#8211; See more at: <a href="http://www.overcomingbias.com/2008/10/the-wire.html#sthash.WTsGobX2.dpuf" rel="nofollow">http://www.overcomingbias.com/2008/10/the-wire.html#sthash.WTsGobX2.dpuf</a></p></blockquote>
<p>To many people, The Wire illustrates all the reasons why capitalism caused the problems of Baltimore and other cities. Any understanding of market forces goes over their head. But to libertarian leaning economists like Robin and <a title="https://twitter.com/atabarrok/status/116247222533894144" href="https://twitter.com/atabarrok/status/116247222533894144">Alex Tabborak</a>, The Wire is obviously illustrating how market forces work and the problems that arise when we ignore these forces. We can give these men fish; they already know calculus.</p>
]]></content:encoded>
							<wfw:commentRss>https://carcinisation.com/2015/01/15/in-order-to-give-a-man-a-fish-you-must-teach-him-how-to-eat-it/feed/</wfw:commentRss>
		<slash:comments>9</slash:comments>
						
		<media:content url="https://0.gravatar.com/avatar/9baac3a725b90101b0d2de3544088f66?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">spoolinator</media:title>
		</media:content>
	</item>
		<item>
		<title>Accidental Trial by Fire</title>
		<link>https://carcinisation.com/2014/10/02/accidental-trial-by-fire/</link>
				<comments>https://carcinisation.com/2014/10/02/accidental-trial-by-fire/#comments</comments>
				<pubDate>Thu, 02 Oct 2014 12:15:02 +0000</pubDate>
		<dc:creator><![CDATA[Matt Simpson]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[anthropology]]></category>
		<category><![CDATA[social cohesion]]></category>
		<category><![CDATA[sociology]]></category>

		<guid isPermaLink="false">http://carcinisation.com/?p=299</guid>
				<description><![CDATA[An Aeon piece by Dimitris Xygalatas has been making the rounds describing how hazing and other rituals involving sacrifice or pain, physical or psychological, often serves as a sort of prosocial glue that keeps groups together and functioning well. Xygalatas goes as far as to measure the heart rates of people involved in a fire walking ritual, &#8230; <a href="https://carcinisation.com/2014/10/02/accidental-trial-by-fire/" class="more-link">Continue reading<span class="screen-reader-text"> "Accidental Trial by&#160;Fire"</span></a>]]></description>
								<content:encoded><![CDATA[<p>An Aeon <a title="http://aeon.co/magazine/society/how-extreme-rituals-bond-us-for-life/" href="http://aeon.co/magazine/society/how-extreme-rituals-bond-us-for-life/">piece</a> by Dimitris Xygalatas has been making the rounds describing how hazing and other rituals involving sacrifice or pain, physical or psychological, often serves as a sort of prosocial glue that keeps groups together and functioning well. Xygalatas goes as far as to measure the heart rates of people involved in a fire walking ritual, and finds that he can predict how closely related two people are in their social network &#8212; e.g. spouse vs. close friend vs. stranger &#8212; just by patterns in their heart rates throughout the night. This insight helps explain vast swaths of social behavior, from fraternity hazing to why people go to concerts. Do read the whole piece for better context and more convincing evidence.</p>
<p>What struck me while reading the piece is how many of these prosocial rituals are almost accidental in our modern lives. We seem to be forming strong social bonds with strangers in a random, haphazard fashion rather an with the people we&#8217;re likely to have extended contact with &#8212; e.g. family or local community members. I can really only speak to my own experience here, so I started listing the various painful &#8220;rituals&#8221; I&#8217;ve participated in to get a feel how true my intuition was. I&#8217;ll list mine below, but I encourage you to list yours in the comments.</p>
<ul>
<li>Enduring long (3 hours +) trips and spending too much money to play in Magic: the Gathering tournaments, often missing other important obligations as a result.</li>
<li>Enduring less long trips and spending even more money to play in paintball tournaments, often missing other important obligations as a result. Also, getting shot is physically painful and the most painful occasions are more likely to occur in these tournaments.</li>
<li>Graduate school &#8211; especially core courses and qualifying exams.</li>
<li>Concerts &#8211; it&#8217;s hot, smoky, hard to see, uncomfortable seating, often had to travel a long distance, etc.</li>
<li>Various outdoor activities (but only sometimes) &#8211; camping, fishing, etc. At their worst, it&#8217;s way too hot or way too cold, it&#8217;s unpleasantly wet, and there&#8217;s sometimes an element of danger. Sometimes they require quite a trip too.</li>
<li>A small underground boxing club in high school</li>
<li>High school itself. School itself.</li>
</ul>
<p>What stands out to me about this list is the number of items that are or are attached to some hobby that requires time, money, and travel and aren&#8217;t typically things you do with your family and non-hobby friends &#8211; how many people are willing to travel for hours to get shot at with paint filled pellets or play a card game? But looking at the list, a smaller proportion of them have the accidental nature than I expected. Traveling and enduring pain for hobbies that few in your family/community participate in does seem like a stereotypically modern behavior. Is anyone aware of any evidence one way or another?</p>
<p>Another thing I noticed is how many are associated with school. School is traumatic in a lot of ways (raise your hand if you&#8217;ve ever had a nightmare about somehow making a huge mistake at school), and ever notice how so many people feel such a strong connection to their alma mater? Perhaps the modern social order is held together by school and hobbies.</p>
]]></content:encoded>
							<wfw:commentRss>https://carcinisation.com/2014/10/02/accidental-trial-by-fire/feed/</wfw:commentRss>
		<slash:comments>3</slash:comments>
						
		<media:content url="https://0.gravatar.com/avatar/9baac3a725b90101b0d2de3544088f66?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">spoolinator</media:title>
		</media:content>
	</item>
		<item>
		<title>The Old Evidence Problem</title>
		<link>https://carcinisation.com/2014/09/26/the-old-evidence-problem/</link>
				<comments>https://carcinisation.com/2014/09/26/the-old-evidence-problem/#comments</comments>
				<pubDate>Fri, 26 Sep 2014 20:50:16 +0000</pubDate>
		<dc:creator><![CDATA[Matt Simpson]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[probability]]></category>
		<category><![CDATA[statistics]]></category>

		<guid isPermaLink="false">http://carcinisation.com/?p=291</guid>
				<description><![CDATA[I&#8217;m in the middle of writing up a post sketching a some ideas I have about Bayesian inference in order to stir up a hornet nest &#8211; in particular to prod the hornet queen, David Chapman. In the process, I ran across this old blog post by Andrew Gelman discussing this (pdf) paper by Bandyopadhyay and &#8230; <a href="https://carcinisation.com/2014/09/26/the-old-evidence-problem/" class="more-link">Continue reading<span class="screen-reader-text"> "The Old Evidence&#160;Problem"</span></a>]]></description>
								<content:encoded><![CDATA[<p>I&#8217;m in the middle of writing up a post sketching a some ideas I have about Bayesian inference in order to stir up a <a title="http://gatherer.wizards.com/Pages/Card/Details.aspx?multiverseid=383267" href="http://gatherer.wizards.com/Pages/Card/Details.aspx?multiverseid=383267">hornet nest</a> &#8211; in particular to prod the <a title="http://gatherer.wizards.com/Pages/Card/Details.aspx?name=hornet+queen" href="http://gatherer.wizards.com/Pages/Card/Details.aspx?name=hornet+queen">hornet queen</a>, David Chapman. In the process, I ran across <a title="http://andrewgelman.com/2013/03/27/two-dogmas-of-strong-objective-bayesianism/" href="http://andrewgelman.com/2013/03/27/two-dogmas-of-strong-objective-bayesianism/">this</a> old blog post by Andrew Gelman discussing <a title="http://www.themattsimpson.com/wp-content/uploads/2014/09/CISP_447079.pdf" href="http://www.themattsimpson.com/wp-content/uploads/2014/09/CISP_447079.pdf">this</a> (pdf) paper by Bandyopadhyay and Brittan criticizing one form of Bayesianism &#8211; in particular the form espoused by E.T. Jaynes. One of the issues they bring up is called the old evidence problem:</p>
<blockquote><p>Perhaps the most celebrated case in the history of science in which old data have been used to construct and vindicate a new theory concerns Einstein. He used Mercury’s perihelion shift (M) to verify the general theory of relativity (GTR). The derivation of M is considered the strongest classical test for GTR. However, according to Clark Glymour’s old evidence problem, Bayesianism fails to explain why M is regarded as<br />
evidence for GTR. For Einstein, Pr(M) = 1 because M was known to be an anomaly for Newton’s theory long before GTR came into being. But Einstein derived M from GTR; therefore, Pr(M|GTR) = 1. Glymour contends that given equation (1), the<br />
conditional probability of GTR given M is therefore the same as the prior probability of GTR; hence, M cannot constitute evidence for GTR.</p></blockquote>
<p>Oh man, do I have some thoughts on this problem. I think I even wrote a philosophy paper in undergrad that touched on it after reading Jaynes. But I&#8217;m going to refrain from commenting until after I finish the main post because I think the old evidence problem illustrates several points that I want to make. In the mean time, what do *you* think of the problem? Is there a solution? What do you think of the solution Bandyopadhyay and Brittan propose in their paper?</p>
<p>Edit: Here&#8217;s a general statement of the problem. Suppose we have some well know piece of evidence E. Everyone is aware of this evidence and there is no doubt, so P(E)=1. Next, suppose someone invents a new theory T that perfectly accounts for the evidence &#8211; it predicts is with 100% accuracy so that P(E|T)=1. Then by Bayes&#8217; rule we have P(T|E)=P(E|T)P(T)/P(E) = P(T), so the posterior and prior are identical and the evidence doesn&#8217;t actually tell us anything about T.</p>
]]></content:encoded>
							<wfw:commentRss>https://carcinisation.com/2014/09/26/the-old-evidence-problem/feed/</wfw:commentRss>
		<slash:comments>15</slash:comments>
						
		<media:content url="https://0.gravatar.com/avatar/9baac3a725b90101b0d2de3544088f66?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">spoolinator</media:title>
		</media:content>
	</item>
		<item>
		<title>The Mark of an Ergodic Crab</title>
		<link>https://carcinisation.com/2014/07/20/the-mark-of-an-ergodic-crab/</link>
				<comments>https://carcinisation.com/2014/07/20/the-mark-of-an-ergodic-crab/#respond</comments>
				<pubDate>Sun, 20 Jul 2014 23:46:28 +0000</pubDate>
		<dc:creator><![CDATA[Matt Simpson]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[math]]></category>
		<category><![CDATA[probability]]></category>
		<category><![CDATA[statistics]]></category>

		<guid isPermaLink="false">http://carcinisation.com/?p=61</guid>
				<description><![CDATA[I. Chasing ergodic crabs @themattsimpson @St_Rev is ergodicity a neat/important thing or a boring thing? &#8212; Sister Sarah (@sarahdoingthing) July 15, 2014 Yes! Ergodicity is interesting and very useful. At least in statistics. Don&#8217;t ask me what physicists do with the thing though. Those guys are crazy. The problem with ergodicity is that it&#8217;s pretty &#8230; <a href="https://carcinisation.com/2014/07/20/the-mark-of-an-ergodic-crab/" class="more-link">Continue reading<span class="screen-reader-text"> "The Mark of an Ergodic&#160;Crab"</span></a>]]></description>
								<content:encoded><![CDATA[<p>I. Chasing ergodic crabs</p>
<div class="embed-twitter">
<blockquote class="twitter-tweet" data-width="550">
<p lang="en" dir="ltr">@themattsimpson <a href="https://twitter.com/St_Rev">@St_Rev</a> is ergodicity a neat/important thing or a boring thing?</p>
<p>&mdash; Sister Sarah (@sarahdoingthing) <a href="https://twitter.com/sarahdoingthing/status/488860010349211648">July 15, 2014</a></p></blockquote>
<p><script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script></div>
<p>Yes! Ergodicity is interesting and very useful. At least in statistics. Don&#8217;t ask me what physicists do with the thing though. Those guys are crazy. The problem with ergodicity is that it&#8217;s pretty complicated. Go ahead, take a look at the Wikipedia <a title="http://en.wikipedia.org/wiki/Ergodicity" href="http://en.wikipedia.org/wiki/Ergodicity">page</a>, I&#8217;ll wait a minute.</p>
<p>&#8230;</p>
<p>&#8230;</p>
<p>Ok, good. The math geniuses are gone now. You know the type &#8211; they can look at some math they&#8217;ve never seen before and understand it in minutes. Right now they&#8217;re busy proving theorems about ergodic flows on n-dimensional locally non-Hausdorff manifolds. And twitching at that last sentence. You and me? We&#8217;re going to learn about ergodicity with a pretty crabby extended metaphor.</p>
<p><span id="more-61"></span></p>
<p>Picture it with me: you&#8217;re on vacation in a small but beautiful coastal town. As you&#8217;re walking along the beach, you notice a crab scuttling about in the surf. Fascinated by crustaceans, you maneuver for a closer look. As you carefully close the distance between you and your new ten-footed friend, he finally acknowledges your presence by pointing his beady little eyes right at you. Undeterred by the tiny eyes of a mere arthropod, you bend down for a closer look. Right at that moment, the crab disappears. His fresh scuttle prints are right there in the sand, but he&#8217;s gone. Nowhere to be found.</p>
<p>For a moment you reflect on the fallibility of your own senses and the inherent meaninglessness of life itself until a sharp clicking sound coming from somewhere behind you snaps you out of your philosophy-induced stupor. When you turn away from the surf, you see another crab playing in the sand. Once you move closer you realize that it&#8217;s not just any crab, it&#8217;s the crab that disappeared! It must have teleported there somehow. You take another step towards the crab and it promptly disappears again, then reappears to your right and clicks its claws at you derisively. Is the crab taunting you? This is no ordinary crab.</p>
<p>Determined to learn more about such an amazing creature, you extend your vacation in order to study its habits. Every morning when you walk out onto to beach, you find Stuart &#8211; you named him Stuart &#8211; hanging out on the beach. He zips around the beach all day, never spending more than a few minutes in one spot.  After a few weeks of observation, you notice that in the morning you tend to find Stuart on the higher peaked sand dunes more often than in the slack between the dunes or in the surf. Stuart really seems to like larger quantities of sand! In fact, when you walk out in the morning the probability that you find him in any given region is precisely the volume of sand in that region as a percentage of the total volume of sand on the beach. But it&#8217;s more than that, of course. This crab teleports! In fact, Stuart doesn&#8217;t scuttle to move around at all. Sure, he&#8217;ll poke around in the sand and shift around a bit in his spot, but to move any significant distance, Stuart teleports. He&#8217;s like Nightcrawler. Except a crab.</p>
<p>Here&#8217;s the weird thing though. When Stuart teleports, the location he teleports to is completely determined by the spot where he teleported from. For example, when Stuart is on the peak of a particularly tall dune on the east side of the beach, he always teleports to a particular spot next to a large rock near the surf. While Stuart only teleports to a single spot from any given spot, there are often multiple spots that he teleports <em>from </em>to get <em>to</em> the same spot. The more sand piled up at a given spot, the more spots on the beach that teleport to that spot. In fact, Stuart&#8217;s teleportations are <em>sand preserving</em>. That is, if you pick a region &#8211; i.e. collection of spots &#8211; on the beach and measure the total volume of sand in that region, the collection of spots that Stuart teleports from that lands him in the region you picked has the same volume! So while Stuart starts at random places every morning, he always follows the same path of spots when he teleports, and when he teleports into a region, it has the same amount of sand as all of the sand on all of the spots that lead into the region.</p>
<p>After studying the behavior of this amazing specimen for some time, you notice another feature of Stuart&#8217;s teleportation scheme &#8211; <em>invariant regions</em>. There are some regions that, once Stuart is there, he never leaves. Specifically, if Stuart is sitting on a spot in an invariant region, he always teleports to another spot in that region; and if he&#8217;s sitting outside of an invariant region, he never teleports into the region. You discovered Stuart&#8217;s invariant regions one day when you managed to finally catch him and set him on a large nearby rock. Inevitably, he teleported away &#8211; but this time to the top of another rock. This was weird because you&#8217;d never seen him teleport to a rock before. So you observed for a little while longer, and he kept teleporting to the top of rocks and not to the sand. The next morning though, back to the sand. But Stuart&#8217;s invariant regions are special. They either contain no sand at all or all of the sand on the beach. For example, the region containing all of the sandy spots but none of the spots on rocks is an invariant region. So is any region containing all of the sandy spots and some rocky spots. Of course, any region containing only rocky spots is also an invariant region. So the probability of finding Stuart in any given invariant region in the morning was either 0 or 1, depending on the region. If it was a region without sand, you never found Stuart there in the morning. If it was a region containing all of the sand, you always found Stuart in the region since he always started off somewhere in the sand.</p>
<p>When you finally end your vacation and show your notes to an arthropod expert in the biology department at the university back home, she&#8217;s unimpressed &#8211; you&#8217;ve merely discovered an <em>ergodic crab</em>. An ergodic crab can get as close as he wants to any sandy spot on the beach from any other sandy spot on the beach so long long as it can teleport enough times. Apparently there&#8217;s another species of crab, a<em> nonergodic crab</em> which has a slightly different teleportation pattern. When you find a nonergodic crab in the morning, sometimes you find him in a given invariant region, then he never leaves that region for the rest of the day. For example, some days you&#8217;ll find a nonergodic crab sitting on wet sand and he&#8217;ll only ever teleport to wet sad that day. On other days, you&#8217;ll find that same crab sitting on dry sand and he&#8217;ll only ever teleport to dry sand that day. So in order to get a feel for where a nonergodic crab tends to hang out, you need to observe him on multiple days. But for an ergodic crab, one day is enough to see how well he likes the various spots on the beach.</p>
<p>There&#8217;s one more feature of sand preserving crabs that you learn from Ginny, your new biologist friend. She tells you that the crabs will click the same number of times each time they arrive in a particular spot. Sure enough, when Stuart teleported to the top of the tallest sand dune on the beach, he always clicked five times. When he teleported to a spot near a large rock just above the surf, Stuart always clicked two times. Between their teleportation paths and clicking patterns, sand preserving crabs are paragons of consistency.</p>
<p>II. Stuart as a mathematical object</p>
<p>The story above translates pretty well into the actual math of ergodicity. The beach is a probability (or measure) space and the distribution of sand on the beach is the distribution of probability (or measure) on the space. A sand preserving crab is a probability (or measure) preserving transformation. Strictly speaking, the crab&#8217;s teleportation pattern is the transformation  &#8211; the transformation sends you to a new spot in the probability space completely determined by where you&#8217;re currently at in the space. The transformation is probability preserving in the same sense that Stuart is sand preserving &#8211; and an invariant subset of the probability space is just an invariant region on the beach. Note that a set is invariant <em>with respect to the transformation.</em> If you change the transformation, you potentially change which sets are invariant. A transformation is ergodic, then, if all of its invariant sets are either probability 1 or probability 0.</p>
<p>Now suppose you apply the transformation over and over again. This gives you a sequence of points in the probability space &#8211; the same idea as a sand preserving crab&#8217;s sequence of spots. Suppose we associate a number with each point in the space, like the number of times Stuart clicks when he arrives at that spot. Then the sequence of numbers is a sequence of random variables. They are random because the first spot on the beach or first spot in the probability space is always chosen randomly. It&#8217;s the transformation that is deterministic. A sequence of random variables constructed this way turns out to be a a <em><a title="http://en.wikipedia.org/wiki/Stationary_process" href="http://en.wikipedia.org/wiki/Stationary_process">stationary</a></em> sequence. The properties of a stationary sequence don&#8217;t change as you move forward and backward in the sequence. Want to know the probability that the 100th variable in the sequence is greater than zero? It&#8217;s the same as the probability that the first variable is greater than zero. What&#8217;s the probability that a sand preserving crab clicks 3 times in his fifth spot? The same as the probability that he clicks 3 times in his first spot. This is to say that sand preserving crabs have stationary clicks. What&#8217;s more, all stationary random variables can be represented using a probability preserving transformation &#8211; all crabs with stationary clicks are sand preserving.</p>
<p>Stationary sequences of random variables are nice because learning about what happened in one part of the sequence tells you a lot about what happens in another part of the sequence, even if you don&#8217;t know much about the precise way in which they&#8217;re related. If a stationary sequence is nonergodic though, you&#8217;re still fundamentally limited in what you can learn about the properties of the sequence. Think about a nonergodic crab teleporting around the beach. If you only ever get to observe the crab on one day, you only ever see it on either wet sand or dry sand. If the crab&#8217;s click rate depends on whether it&#8217;s on dry or wet sand and you only ever see it on wet sand, how can you learn about it&#8217;s click rate on dry sand? You can&#8217;t &#8211; at least not unless you know the relationship between the crab&#8217;s behavior on wet sand and its behavior on dry sand.</p>
<p>Let&#8217;s view probability from a <a title="http://en.wikipedia.org/wiki/Frequentist_probability" href="http://en.wikipedia.org/wiki/Frequentist_probability">frequentist</a> perspective for a moment &#8211; i.e. let&#8217;s allow ourselves to view a real world sequence of variables as random. We only ever get to see one realization of many sequences of random variables. We only get to see the inflation rate in 1972 once, for example. The next time we see an inflation rate, it&#8217;s a different year. So even if inflation follows a stationary process, if we believe it&#8217;s nonergodic we&#8217;re fundamentally limited in what we can learn about that process. When it comes to ergodic processes, though, a single realization of the sequence will tell you everything you want to know about about the process, as long as you watch long enough. If you watch an ergodic crab long enough, he&#8217;ll make his way all over the beach. For a nonergodic crab, you need to keep coming back day after day until you&#8217;ve had at least one day with him on wet sand and one day with him on dry sand. But in the real world, you&#8217;re often limited to only one day.</p>
<p>If the sequence &#8211; or crab &#8211; is nonstationary, you&#8217;re even worse off. If a nonergodic sequence is stationary, at least today&#8217;s value still tells you something about tomorrow&#8217;s value. If the sequence is also nonstationary, you need to have some additional information about the process in order for today&#8217;s value to give you any information about tomorrow&#8217;s value. And often we don&#8217;t.</p>
<p>From a <a title="http://en.wikipedia.org/wiki/Bayesian_probability" href="http://en.wikipedia.org/wiki/Bayesian_probability">Bayesian</a> perspective, probability is in the mind, not reality, so none of this makes much sense. Probability is uncertainty about reality, not reality itself. But I don&#8217;t think there&#8217;s a Bayesian free lunch here. Another way to think about ergodicity is that in an ergodic sequence of random variables, the structure and amount of dependence is limited to a large degree. For example, if today&#8217;s inflation rate depends on the entire history of inflation rates, then 1) older inflation rates are much less important than more recent rates to predicting today&#8217;s rate and 2) how today&#8217;s inflation rate depends on the older inflation rates is limited to a small set of relatively simple possibilities. From a Bayesian perspective, a nonergodic sequence of random variables is a sequence where all we know about the relationship between today&#8217;s value and all previous values is that it&#8217;s either not simple in the sense of the previous sentence or older values are indeed very important for predicting the next value of the sequence. In short, variables in a nonergodic sequence have complicated relationships. But because of the nature of this complication, we it&#8217;s hard to learn about the relationship between the variables in the sequence &#8211; at least not without more information. We can model the relationship in one way or another, but without a reason to favor any given model, we can&#8217;t take the conclusions we draw using any given model seriously.</p>
<p>Whether you&#8217;re a Bayesian or a frequentist, ergodicity and stationarity are often used in pieces of a larger model that is nonergodic or nonstationary. Often this helps develop plausible models because it&#8217;s easier to argue that the various components of the model are stationary &#8211; it&#8217;s essentially a strategy for arguing for one form of nonergodicity or another. How successful this strategy is depends on the particular application, but it can often be a helpful if not perfect exercise.</p>
<p>The way I&#8217;ve been using the term &#8220;ergodic&#8221; so far isn&#8217;t always how it&#8217;s used though. One property of an ergodic sequence of random variables is that if you take the average of the sequence over time, this average will converge to the true average. We sometimes say that the variable has an ergodic mean when this happens. Often, the whole business of measure preserving transformations is swept under the rug and a random variable is defined as ergodic if it has an ergodic mean. From a frequentist perspective, this makes a ton of sense. Suppose each random variable in the sequence has the same true mean &#8211; the &#8220;space mean&#8221; since it&#8217;s the mean of the variable, averaged over the whole probability space. Since we can only ever observe one realization of the sequence, the hope is that the &#8220;time mean,&#8221; or the mean of a given realization of the sequence, will estimate the space mean. When the sequence has an ergodic mean, this is true. In practice frequentist statistics is all about ensuring that a sequence of estimates converge to the true value. In addition, there are some nonergodic and even nonstationary sequences of random variables which still have ergodic means, which means the frequentist can still estimate that mean. So you can understand why they often care more about ergodic means than ergodic transformations.</p>
<p>The bottom line is that ergodicity makes learning from the data much, much easier no matter your favorite school of probability. In a domain with nonergodic data, it&#8217;s that much harder to come up with a model that closely resembles the truth, which means any inferences that you draw are that much more questionable. And there are good reasons to think that lots of data is nonergodic, especially in the social sciences. So good luck economists.</p>
<p>III. Markov crab Monte Carlo</p>
<p>Another area of statistics where ergodicity is incredibly useful is in statistical computation. But in order to understand this, we have to take a quick detour through the practice of Bayesian statistics. We start, of course, with <a title="http://en.wikipedia.org/wiki/Bayes'_theorem" href="http://en.wikipedia.org/wiki/Bayes'_theorem">Bayes&#8217; rule</a>. There are a couple competing ways to think about Bayesian statistics, but we&#8217;ll think about a particularly common one mostly because it&#8217;s easier.</p>
<p>Suppose you have some model for how the data are generated, represented by the probability distribution p(Y_1, Y_2, &#8230;, Y_T|X), assuming we have T observations of the variable Y in our dataset. We&#8217;ll just use p(Y|X) for short, i.e. Y = (Y_1, Y_2, &#8230;, Y_T). Here, X is some parameter which we don&#8217;t know. Maybe it&#8217;s the mean of the Y&#8217;s, for example. Y is the data you&#8217;ve already observed. Then p(Y|X) asks you for a value of X, then gives you a probability distribution on this data that depends on the value you input. Since we don&#8217;t know X and we&#8217;re Bayesian, we represent our uncertainty about X in another probability distribution called the <em>prior</em>, p(X). This thing traces out a curve above zero by plugging in different values of X. The probability that X is in a given region is the area under that curve but within that region. The distribution p(Y|X) is similar, except the probability that Y is in a given region depends on the value of X you give to it &#8211; it&#8217;s a conditional distribution, so each value of X determines a different curve for Y.</p>
<p>What we want to learn about in this example is X &#8211; after seeing the data, we want to know what our uncertainty about X looks like. This is another conditional distribution, called the <em>posterior</em> distribution, p(X|Y). If you input a particular value for the data you&#8217;ve observed, p(X|Y) spits out a new probability distribution on X reflecting what you observed. Bayes&#8217; rule tells us that p(X|Y) = p(Y|X) * p(X) / p(Y). In most situations, we can write down p(Y|X) and p(X), so multiplying them together is no problem. p(Y) is a little different &#8211; it represents our uncertainty about the data alone (i.e. without considering the relationship between Y and anything else), in the form of a probability distribution. This thing is maddeningly difficult to calculate in all but the most simple models. So we don&#8217;t try to compute it at all, at least not by sitting down with a pen and paper and trying to derive it analytically.</p>
<p>An alternative to actually computing p(Y) analytically is called <a title="http://en.wikipedia.org/wiki/Monte_Carlo_integration" href="http://en.wikipedia.org/wiki/Monte_Carlo_integration">Monte Carlo simulation</a>. The idea here is that if we can&#8217;t analytically derive the posterior distribution, maybe we know enough to draw random samples from it using a computer with a random number generator. Given a large enough random sample from the posterior, there&#8217;s no difference from actually being able to analytically derive it &#8211; we can approximate the posterior probability that X is in any region and that approximation gets better as our random sample gets larger.</p>
<p>In practice, it&#8217;s pretty hard to effectively use Monte Carlo simulation in order to sample from a posterior distribution. The main problem is that it would take forever to get a large enough sample that we can say anything meaningfully accurate about the posterior distribution. But what if instead of directly sampling from the posterior distribution, we only sampled from it approximately? This is exactly what <a title="http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo" href="http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markov chain Monte Carlo simulation</a> does, though there other ways to construct an approximation.</p>
<p>A <a title="http://en.wikipedia.org/wiki/Markov_chain" href="http://en.wikipedia.org/wiki/Markov_chain">Markov chain</a> is a special sequence of random variables where dependence is allowed, but only a specific kind of dependence. Specifically, the next value in the chain is allowed to depend on the current value in arbitrarily complicated ways, but oncee you know the current value none of the older values tell you anything about the next value. In order to predict what&#8217;s going to happen tomorrow, you need to know what happened today. But once you do, yesterday is irrelevant.</p>
<p>In a Markov chain Monte Carlo simulation, or MCMC, you construct a Markov chain for X on your computer. You give the chain an initial value, X_0. Then the chain randomly simulates a new value, X_1. Next you feed X_1 back into the chain and it spits out a new simulated value, X_2. And so on. Back on the beach, we can think about a Markov crab, named <a title="http://en.wikipedia.org/wiki/Andrey_Markov" href="http://en.wikipedia.org/wiki/Andrey_Markov">Andrey</a> of course. Imagine you get to hear Andrey&#8217;s claws clicking every time he teleports, but you don&#8217;t get to see where he teleports. Maybe you&#8217;re too busy checking out your neighbor on the beach to actually look at the crab. But you do hear Andrey every time he clicks his claws. Since Andrey is a Markov crab, the probability that he clicks 3 times the next time he teleports depends <em>only </em>on how many times he clicked the last time he teleported and, once you know that fact, none of his previous clicks are relevant.</p>
<p>In order for MCMC to work though, we can&#8217;t have any old Markov <del>crab</del> chain. As we go farther into the sequence of X_t &#8216;s given by the Markov chain, we need the probability distribution of X_t to get closer and closer to the probability distribution of X, i.e. the posterior distribution we wanted to sample from in the first place. In other words, we need p(X_t) to get closer and closer to p(X) as t goes towards infinity. Then with a long enough sample from the Markov chain, we can again approximate any posterior probability, with a better approximation as the sample gets larger.</p>
<p>One of the conditions that will help guarantee that the approximation works is that the Markov chain is ergodic. Think about the beach again. Suppose Andrey is whizzing about the beach with his magical teleportation power. If Andrey is a nonergodic crab then once he starts his chain, he&#8217;ll only ever be on the type of sand he started on &#8211; dry or wet. If our posterior distribution of clicks depends in some way on the type of sand the crab is clicking from, then Andrey&#8217;s distribution of clicks may never approach the posterior. If Andrey&#8217;s an ergodic crab though, there&#8217;s hope. No matter where Andrey starts, he&#8217;ll explore the entire beach if we just wait long enough. So over time the sample distribution of Andrey&#8217;s  clicks will become a better and better approximation of the posterior distribution we care about.</p>
<p>Back to our computer simulations, we need to make sure that the Markov chain for X never gets stuck in some region of X&#8217;s space. If the chain only ever spits out values of X that are above zero, we&#8217;re in trouble. If the chain is ergodic though, this can&#8217;t happen. Ergodicity alone isn&#8217;t enough to guarantee that a Markov chain will approximate our posterior distribution &#8211; it guarantees that the probability distribution of a Markov chain converges to <em>something</em>, but not necessarily that it converges to our posterior. This isn&#8217;t typically a major difficulty. Usually, the big problem in MCMC comes from trying to find a Markov chain which converges to the posterior distribution timely manner. This inches us pretty close to my research area, but it also takes us pretty far away from ergodicity. So I think I&#8217;ll just stop here before I coerce anymore crabs into another metaphor.</p>
]]></content:encoded>
							<wfw:commentRss>https://carcinisation.com/2014/07/20/the-mark-of-an-ergodic-crab/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
						
		<media:content url="https://0.gravatar.com/avatar/9baac3a725b90101b0d2de3544088f66?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">spoolinator</media:title>
		</media:content>
	</item>
		<item>
		<title>What is statistics?</title>
		<link>https://carcinisation.com/2014/07/14/what-is-statistics/</link>
				<comments>https://carcinisation.com/2014/07/14/what-is-statistics/#respond</comments>
				<pubDate>Tue, 15 Jul 2014 06:17:59 +0000</pubDate>
		<dc:creator><![CDATA[Matt Simpson]]></dc:creator>
				<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[statistics]]></category>

		<guid isPermaLink="false">http://carcinisation.com/?p=30</guid>
				<description><![CDATA[I put the call out on twitter for ideas for my first post, and Gabe asked this: https://twitter.com/GabrielDuquette/status/488858307885424640 I suppose I did ask for statistics questions. This one is a bit tough to answer because, like I hinted at on twitter, a wide variety of things get called statistics by the people doing them, statisticians &#8230; <a href="https://carcinisation.com/2014/07/14/what-is-statistics/" class="more-link">Continue reading<span class="screen-reader-text"> "What is statistics?"</span></a>]]></description>
								<content:encoded><![CDATA[<p>I put the call out on twitter for ideas for my first post, and Gabe asked this:</p>
<p><a href="https://twitter.com/GabrielDuquette/status/488858307885424640" rel="nofollow">https://twitter.com/GabrielDuquette/status/488858307885424640</a></p>
<p>I suppose I did ask for statistics questions. This one is a bit tough to answer because, like I hinted at on twitter, a wide variety of things get called statistics by the people doing them, statisticians do an even wider variety of things, and to muddy the waters even more, lots of things that typically get categorized as statistics often are also categorized as other things like machine learning or computer science. I suppose I should blame the computer people.</p>
<p><span id="more-30"></span></p>
<p>To get a sense for the word, we can start at <a title="http://en.wikipedia.org/wiki/Statistics" href="http://en.wikipedia.org/wiki/Statistics">Wikipedia</a>:</p>
<blockquote><p>Statistics is the study of the collection, organization, analysis, interpretation and presentation of data.</p></blockquote>
<p>So statistics is about data, that much can be sure. The first question of the first test in the Stat 104 course I taught asked</p>
<blockquote><p>Statistics is about __________________.</p></blockquote>
<p>If you would have written in &#8220;data&#8221; I would have marked it wrong. So much for data, eh? The <del>password</del> answer I wanted in that blank, just like the answer my predecessors wanted in the long storied history of Stat 104, was variation. If you guessed that the reason I wanted that answer was because I got lecture notes, assignments and example tests from a previous instructor, you&#8217;d be right. But I realized while teaching that course that there was something to that definition. Pretty much everything statisticians do with data has something to do with analyzing, organizing, interpreting and presenting the <em>variation</em> in data. Or lack thereof. The central focus of classical statistics is variation. Suppose you have some variable you care about. Classical statistics basically breaks down the variation in that variable into two components &#8211; 1) variation that can be explained by a given model and 2) variation that can&#8217;t, i.e. the error. Then using these components, you try to answer all sorts of interesting questions as long as you can phrase them as questions about the model.</p>
<p>But wait, what is a model? This question gets at the heart of what statistics is all about. There are lots of peripheral things that people do that can be classified as statistics without much fuss, but they&#8217;re, well, peripheral. The key component tying most of the field together is probability theory. We can&#8217;t even get statisticians to agree on what probability is, but by and large everyone agrees on the math. And this math is used to build models. Models of what? Well, that depends on who you ask. A frequentist might tell you we&#8217;re building models of the data generating process. A Bayesian might tell you we&#8217;re building models of our uncertainty about the data generating process. A pragmatist might refuse to answer the question and scuttle away. But we build models and those models use probability.</p>
<p>Lots of disparate things under the statistics umbrella are tied to the center by probability. Statisticians use probability to think about data collection and experimental design. Data visualization and model construction are mutually reinforcing &#8211; we use plots to help select useful models and models to help come up with useful plots. Statistical computing is driven by the need to fit models faster and to this end often uses concepts from probability. Mathematical statistics is basically the derivation of high-falutin&#8217; probability theory relevant to statistical problems.</p>
<p>So statistics is about data and using probability to understand variation in the data. Except when it isn&#8217;t. <a title="http://normaldeviate.wordpress.com/2012/07/04/statistics-without-probability-individual-sequences/" href="http://normaldeviate.wordpress.com/2012/07/04/statistics-without-probability-individual-sequences/">Probability free statistics</a> looks a lot like frequentist statistics, except it ignores probability theory entirely. So while probability theory isn&#8217;t essential, it&#8217;s pretty close. Probability free statistics is also a good test case for figuring out the other essential features of statistics. In probability free statistics, you are given a sequence of observations and you attempt to predict the next observation in the sequence. Given a set of different prediction algorithms, using what amounts to worst-case thinking in decision theory you can show that the best prediction is some sort of weighted average of the predictions of the original set.</p>
<p>Outside of using data, the two keys are prediction and decision theory. Prediction is a common topic in statistics, but that&#8217;s not what makes probability free statistics statistics. It&#8217;s actually decision theory. Well, statistical decision theory, but it&#8217;s really just normal decision theory with a different emphasis. Most statistical methods can be justified using decision theory. Bayesians see decision theory as an essential component of statistics &#8211; some don&#8217;t think probability can even be defined apart from decision theory! But the statement is true for frequentists as well. Prediction is a good example of this. You see a sequence of coin flips and have to predict the next flip. You choose between heads and tails. Which one should you choose? The one that maximizes utili&#8212;ahem. The one that minimizes loss. Or worst case loss. Or expected loss. (Psst. Loss is the negative of utility.) What about p-values, what does decision theory have to do with p-values? You have to choose whether or not to reject the null hypothesis! P-values are a component to the solution of that decision problem. Parameter estimation? Decision theory. Confidence intervals? Decision theory. Model selection? Decision theory. Picking the best way to display your data? Maybe that doesn&#8217;t seem like decision theory, but if you give me 10 minutes and a whiteboard&#8230; Ok, fine, probably not decision theory.</p>
<p>Alright, decision theory is a big part of statistics. But is it an essential part? I&#8217;m not sure. This seems like the key idea we were missing when I brought up probability free statistics, but it probably isn&#8217;t essential. I&#8217;m inclined to say that while neither probability theory nor decision theory is essential, at least one is required for something to be &#8220;statistics.&#8221; But I won&#8217;t say that because there might be an example of something we call statistics or looks a lot like statistics that doesn&#8217;t use probability theory or decision theory. I&#8217;m not going to throw my hands up and say that statistics is just what statisticians do though &#8211; there may be edge cases, but there&#8217;s still a big cluster over there in idea space that needs a name. So bottom line, what is statistics? I think Wikipedia was close but a little too inclusive &#8211; probability theory and decision theory are both important if not essential. We can fix that right up though:</p>
<blockquote><p>Statistics is the study of the collection, organization, analysis, interpretation and presentation of data, especially through the frameworks of probability theory and statistical decision theory.</p></blockquote>
<p>&nbsp;</p>
]]></content:encoded>
							<wfw:commentRss>https://carcinisation.com/2014/07/14/what-is-statistics/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
						
		<media:content url="https://0.gravatar.com/avatar/9baac3a725b90101b0d2de3544088f66?s=96&#38;d=identicon&#38;r=G" medium="image">
			<media:title type="html">spoolinator</media:title>
		</media:content>
	</item>
	</channel>
</rss>
